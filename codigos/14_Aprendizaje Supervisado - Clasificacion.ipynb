{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Clasificación\n",
    "\n",
    "La clasificación es una técnica de aprendizaje supervisado en la que el objetivo es predecir la categoría a la que pertenece una observación en función de sus características. Algunos ejemplos de problemas de clasificación incluyen:\n",
    "- Diagnóstico médico (e.g., determinar si una célula es cancerosa o no).\n",
    "- Clasificación de correos electrónicos (e.g., identificar si un correo es spam o no).\n",
    "- Reconocimiento de imágenes (e.g., identificar el contenido de una imagen).\n",
    "\n",
    "### Evaluación de Modelos de Clasificación\n",
    "Para evaluar el rendimiento de un modelo de clasificación, utilizamos varias métricas, incluyendo:\n",
    "- **Matriz de Confusión:** una tabla que muestra las predicciones correctas e incorrectas desglosadas por clase.\n",
    "- **Precisión:** la proporción de verdaderos positivos sobre el total de predicciones positivas.\n",
    "- **Recall (Sensibilidad):** la proporción de verdaderos positivos sobre el total de verdaderos positivos y falsos negativos.\n",
    "- **F1-score:** la media armónica de la precisión y el recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar el dataset de Cáncer de Mama de Wisconsin\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Visualizar las primeras filas del dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de Datos\n",
    "\n",
    "Antes de aplicar cualquier algoritmo de clasificación, es crucial preparar adecuadamente los datos. Esto incluye la normalización y estandarización de los datos, así como la codificación de variables categóricas si es necesario.\n",
    "\n",
    "## Normalización y Estandarización\n",
    "\n",
    "La normalización y estandarización son técnicas para escalar los datos, lo cual puede mejorar el rendimiento de algunos algoritmos de clasificación.\n",
    "\n",
    "- **Normalización:** Escala los valores de las características para que estén en un rango de 0 a 1.\n",
    "- **Estandarización:** Escala los valores de las características para que tengan media 0 y desviación estándar 1.\n",
    "\n",
    "## One Hot Encoding\n",
    "\n",
    "Si se tienen variables categóricas, es necesario convertirlas a una forma numérica que los algoritmos puedan utilizar. One Hot Encoding es una técnica que convierte cada categoría en una columna binaria (0 o 1).\n",
    "\n",
    "## División de los Datos\n",
    "\n",
    "Es fundamental dividir los datos en conjuntos de entrenamiento y prueba para evaluar adecuadamente el rendimiento del modelo. Usualmente se usa una proporción de 70-30 o 80-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización y estandarización\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualizar la forma de los conjuntos de datos\n",
    "print(f'Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras')\n",
    "print(f'Tamaño del conjunto de prueba: {X_test.shape[0]} muestras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística\n",
    "\n",
    "La regresión logística es un algoritmo de clasificación que se utiliza para predecir la probabilidad de que una observación pertenezca a una de dos categorías. Aunque se llama \"regresión\", en realidad es una técnica de clasificación.\n",
    "\n",
    "## Parámetros del Modelo de Regresión Logística\n",
    "\n",
    "En scikit-learn, podemos modificar varios parámetros importantes del modelo de regresión logística para ajustar su comportamiento:\n",
    "\n",
    "- `penalty`: Este parámetro especifica el tipo de penalización que se aplicará para evitar el sobreajuste (cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos). Las opciones son 'l1', 'l2', 'elasticnet' o 'none'. La penalización 'l2' es la predeterminada y es la más comúnmente usada.\n",
    "\n",
    "- `C`: Este parámetro controla la regularización, que es una técnica para evitar el sobreajuste. Un valor más pequeño de `C` significa una regularización más fuerte, lo que puede hacer que el modelo sea más simple y menos propenso a sobreajustarse.\n",
    "\n",
    "- `solver`: Este parámetro especifica el algoritmo que se usará para encontrar los mejores parámetros del modelo. Diferentes algoritmos tienen diferentes eficiencias dependiendo del tamaño del conjunto de datos y el tipo de penalización. Las opciones incluyen 'newton-cg', 'lbfgs', 'liblinear', 'sag' y 'saga'.\n",
    "\n",
    "- `max_iter`: Este parámetro establece el número máximo de iteraciones que el algoritmo de optimización realizará. Si el modelo no converge (no encuentra los mejores parámetros) en el número predeterminado de iteraciones (100), podemos aumentar este valor.\n",
    "\n",
    "## Cómo Funciona el Algoritmo\n",
    "\n",
    "1. **Inicialización de Parámetros:**\n",
    "   Al principio, el modelo empieza con valores iniciales para los parámetros (pesos). Estos parámetros se ajustarán durante el proceso de entrenamiento para mejorar las predicciones.\n",
    "\n",
    "2. **Cálculo de la Probabilidad:**\n",
    "   La regresión logística usa una función llamada sigmoide para calcular la probabilidad de que una muestra pertenezca a la clase positiva (por ejemplo, si una célula es cancerosa o no). La fórmula de la función sigmoide es:\n",
    "\n",
    "   $$\n",
    "   \\text{sigmoide}(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   Donde $z$ es una combinación lineal de las características de entrada y los parámetros (pesos) del modelo.\n",
    "\n",
    "3. **Función de Costo:**\n",
    "   Para saber qué tan bien está funcionando el modelo, usamos una función de costo que mide el error entre las predicciones del modelo y las etiquetas verdaderas. El objetivo es minimizar este error ajustando los parámetros del modelo. Aquí es donde el parámetro `penalty` y `C` juegan un papel importante.\n",
    "\n",
    "4. **Optimización:**\n",
    "   El modelo ajusta los parámetros utilizando un proceso de optimización llamado gradiente descendente (o alguno de los otros solvers especificados). Este proceso ajusta los pesos poco a poco para minimizar el error. El parámetro `solver` define cuál algoritmo de optimización se utilizará, y `max_iter` define cuántas iteraciones se permiten para encontrar la mejor solución.\n",
    "\n",
    "5. **Predicción:**\n",
    "   Después de entrenar el modelo, usamos la función sigmoide para calcular la probabilidad de que una nueva muestra pertenezca a la clase positiva. Si la probabilidad es mayor o igual a 0.5, la muestra se clasifica como positiva; de lo contrario, se clasifica como negativa.\n",
    "\n",
    "### Visualización del Algoritmo de Regresión Logística\n",
    "\n",
    "Para entender mejor cómo funciona la regresión logística, podemos visualizar el proceso de clasificación en un espacio bidimensional utilizando un conjunto de datos sintético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Crear un conjunto de datos sintético\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "# Crear el modelo de regresión logística\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Visualización del conjunto de datos y la clasificación\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
    "\n",
    "# Crear un grid de puntos para visualizar la frontera de decisión\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Dibujar la frontera de decisión\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "plt.title('Visualización del Algoritmo de Regresión Logística')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en scikit-learn\n",
    "\n",
    "A continuación, implementaremos la regresión logística utilizando scikit-learn y ajustaremos algunos de estos parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase LogisticRegression de scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Crear el modelo de regresión logística con algunos parámetros ajustados\n",
    "logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=10000)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con los datos de prueba\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo\n",
    "\n",
    "Después de entrenar y realizar predicciones con el modelo de regresión logística, es importante evaluar su rendimiento. Dos herramientas comunes para esto son la **matriz de confusión** y el **reporte de clasificación**.\n",
    "\n",
    "### Matriz de Confusión\n",
    "\n",
    "La matriz de confusión es una tabla que se utiliza para describir el rendimiento de un modelo de clasificación. Cada fila de la matriz representa las instancias de una clase real, mientras que cada columna representa las instancias de una clase predicha. Aquí se muestran cuatro valores clave:\n",
    "\n",
    "- **Verdaderos Positivos (TP):** Número de observaciones positivas correctamente predichas.\n",
    "- **Verdaderos Negativos (TN):** Número de observaciones negativas correctamente predichas.\n",
    "- **Falsos Positivos (FP):** Número de observaciones negativas incorrectamente predichas como positivas.\n",
    "- **Falsos Negativos (FN):** Número de observaciones positivas incorrectamente predichas como negativas.\n",
    "\n",
    "### Reporte de Clasificación\n",
    "\n",
    "El reporte de clasificación proporciona una descripción más detallada del rendimiento del modelo a través de varias métricas:\n",
    "\n",
    "- **Precisión:** La proporción de predicciones positivas correctas sobre el número total de predicciones positivas realizadas. Calculada como \n",
    "$$ \\frac{TP}{TP + FP} $$\n",
    "- **Recall (Sensibilidad):** La proporción de predicciones positivas correctas sobre el número total de observaciones positivas reales. Calculada como \n",
    "$$ \\frac{TP}{TP + FN} $$\n",
    "- **F1-Score:** La media armónica de la precisión y el recall, proporcionando un balance entre ambos. Calculada como \n",
    "$$ 2 \\cdot \\frac{\\text{Precisión} \\cdot \\text{Recall}}{\\text{Precisión} + \\text{Recall}} $$\n",
    "- **Soporte (Support):** El número de ocurrencias de cada clase en el conjunto de datos de prueba.\n",
    "\n",
    "A continuación, se muestra cómo evaluar el modelo utilizando la matriz de confusión y el reporte de clasificación en scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(cm)\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (K-NN)\n",
    "\n",
    "El algoritmo K-Nearest Neighbors (K-NN) es un método de clasificación sencillo y eficaz. Clasifica una muestra basada en la mayoría de los \"vecinos\" más cercanos a esa muestra en el conjunto de entrenamiento. En otras palabras, asigna una clase a la muestra en función de la clase más común entre sus K vecinos más cercanos.\n",
    "\n",
    "## Parámetros del Modelo K-NN\n",
    "\n",
    "En scikit-learn, podemos modificar varios parámetros importantes del modelo K-NN para ajustar su comportamiento:\n",
    "\n",
    "- `n_neighbors`: Número de vecinos a considerar para la clasificación. Por defecto es 5.\n",
    "- `weights`: Método para ponderar las contribuciones de los vecinos. Las opciones son 'uniform' (todos los vecinos contribuyen por igual) y 'distance' (los vecinos más cercanos tienen mayor peso).\n",
    "- `algorithm`: Algoritmo utilizado para calcular los vecinos más cercanos ('auto', 'ball_tree', 'kd_tree', 'brute'). El valor por defecto es 'auto', que intenta determinar el mejor algoritmo basado en el conjunto de datos.\n",
    "- `p`: Potencia del parámetro de la métrica de distancia de Minkowski. Por defecto es 2, que corresponde a la distancia euclidiana.\n",
    "\n",
    "## Cómo Funciona el Algoritmo\n",
    "\n",
    "1. **Inicialización de Parámetros:**\n",
    "   El modelo comienza con los valores iniciales para los parámetros, como el número de vecinos (K).\n",
    "\n",
    "2. **Cálculo de Distancias:**\n",
    "   Para clasificar una nueva muestra, el algoritmo calcula la distancia entre esta muestra y todas las muestras en el conjunto de entrenamiento. La distancia más comúnmente utilizada es la distancia euclidiana.\n",
    "\n",
    "3. **Identificación de Vecinos:**\n",
    "   Una vez calculadas las distancias, el algoritmo identifica los K vecinos más cercanos a la muestra.\n",
    "\n",
    "4. **Asignación de Clase:**\n",
    "   La clase de la muestra se determina por la clase más frecuente entre los K vecinos más cercanos. Si se utiliza `weights='distance'`, los vecinos más cercanos tienen un mayor peso en la decisión.\n",
    "\n",
    "### Visualización del Algoritmo K-NN\n",
    "\n",
    "Para entender mejor cómo funciona K-NN, podemos visualizar el proceso de clasificación en un espacio bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Crear un conjunto de datos de ejemplo\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
    "\n",
    "# Ajustar un modelo K-NN con k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Visualización del conjunto de datos y la clasificación\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
    "\n",
    "# Crear un grid de puntos para visualizar la frontera de decisión\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Dibujar la frontera de decisión\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "plt.title('Visualización del Algoritmo K-NN')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en scikit-learn\n",
    "\n",
    "A continuación, implementaremos el algoritmo K-NN utilizando scikit-learn y ajustaremos algunos de estos parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase KNeighborsClassifier de scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Crear el modelo K-NN con algunos parámetros ajustados\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con los datos de prueba\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(cm)\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) es un potente algoritmo de clasificación que busca encontrar el hiperplano que mejor separa las clases en el espacio de características. Este hiperplano se define de manera que la distancia (margen) entre las clases y el hiperplano sea la mayor posible.\n",
    "\n",
    "## Parámetros del Modelo SVM\n",
    "\n",
    "En scikit-learn, podemos modificar varios parámetros importantes del modelo SVM para ajustar su comportamiento:\n",
    "\n",
    "- `C`: Parámetro de regularización. Controla el equilibrio entre maximizar el margen y minimizar el error de clasificación. Un valor más pequeño de `C` crea un margen más amplio, a costa de más errores de clasificación.\n",
    "- `kernel`: Especifica el tipo de kernel a usar en el algoritmo. Las opciones incluyen 'linear', 'poly' (polinomial), 'rbf' (función de base radial), y 'sigmoid'.\n",
    "- `gamma`: Parámetro del kernel para 'rbf', 'poly' y 'sigmoid'. Define cómo influye cada punto de entrenamiento en el modelo. Un valor alto de `gamma` significa una influencia alta de cada punto de entrenamiento, lo que puede llevar a sobreajuste.\n",
    "\n",
    "## Cómo Funciona el Algoritmo\n",
    "\n",
    "1. **Inicialización de Parámetros:**\n",
    "   Se configuran los parámetros iniciales del modelo, incluyendo el tipo de kernel y el parámetro de regularización `C`.\n",
    "\n",
    "2. **Cálculo del Hiperplano:**\n",
    "   El algoritmo SVM encuentra el hiperplano que mejor separa las clases en el espacio de características. Este hiperplano maximiza la distancia entre las muestras más cercanas de cada clase (vectores de soporte) y el hiperplano.\n",
    "\n",
    "3. **Kernel Trick:**\n",
    "   Para problemas no lineales, SVM utiliza una función de kernel para transformar el espacio de características a una dimensión superior donde las clases sean separables linealmente.\n",
    "\n",
    "4. **Optimización:**\n",
    "   Se utiliza un algoritmo de optimización para ajustar el hiperplano de manera que se maximice el margen y se minimice el error de clasificación, teniendo en cuenta el parámetro de regularización `C`.\n",
    "\n",
    "5. **Predicción:**\n",
    "   Una vez entrenado el modelo, se puede utilizar el hiperplano para clasificar nuevas muestras.\n",
    "\n",
    "### Visualización del Algoritmo SVM\n",
    "\n",
    "Para entender mejor cómo funciona SVM, podemos visualizar el proceso de clasificación en un espacio bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Crear un conjunto de datos sintético\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "# Crear el modelo SVM con un kernel lineal\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Visualización del conjunto de datos y la clasificación\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
    "\n",
    "# Crear un grid de puntos para visualizar la frontera de decisión\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Dibujar la frontera de decisión\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "plt.title('Visualización del Algoritmo SVM')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en scikit-learn\n",
    "\n",
    "A continuación, implementaremos el algoritmo SVM utilizando scikit-learn y ajustaremos algunos de estos parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase SVC de scikit-learn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Crear el modelo SVM con un kernel 'rbf' y algunos parámetros ajustados\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con los datos de prueba\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(cm)\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En esta libreta, hemos explorado tres algoritmos fundamentales de clasificación en el aprendizaje automático utilizando la biblioteca scikit-learn: la Regresión Logística, K-Nearest Neighbors (K-NN) y Support Vector Machines (SVM). A continuación, resumimos los puntos clave de cada uno:\n",
    "\n",
    "## Resumen de los Algoritmos de Clasificación\n",
    "\n",
    "1. **Regresión Logística:**\n",
    "   - Es un método de clasificación binaria que utiliza la función sigmoide para calcular la probabilidad de pertenencia a una clase.\n",
    "   - Es adecuado para problemas donde la relación entre las características y la variable objetivo es lineal.\n",
    "   - La regularización ayuda a prevenir el sobreajuste, y su efectividad puede ajustarse con el parámetro `C`.\n",
    "\n",
    "2. **K-Nearest Neighbors (K-NN):**\n",
    "   - Clasifica una muestra basada en la mayoría de los \"vecinos\" más cercanos a esa muestra en el conjunto de entrenamiento.\n",
    "   - No hace suposiciones sobre la distribución de los datos, pero puede ser computacionalmente costoso para conjuntos de datos grandes.\n",
    "   - Los parámetros `n_neighbors`, `weights` y `algorithm` permiten ajustar el comportamiento del modelo.\n",
    "\n",
    "3. **Support Vector Machines (SVM):**\n",
    "   - Encuentra el hiperplano que mejor separa las clases en el espacio de características, maximizando el margen entre las clases.\n",
    "   - Utiliza diferentes tipos de kernels (lineal, polinomial, RBF) para manejar relaciones no lineales entre características.\n",
    "   - Los parámetros `C` y `gamma` son cruciales para ajustar el modelo y prevenir el sobreajuste.\n",
    "\n",
    "## Consejos para la Selección de Modelos\n",
    "\n",
    "- **Conocimiento del Problema:** La elección del modelo debe basarse en la comprensión del problema y la naturaleza de los datos. Por ejemplo, la regresión logística es adecuada para relaciones lineales, mientras que SVM puede manejar relaciones no lineales complejas.\n",
    "- **Evaluación de Modelos:** Siempre es importante evaluar el rendimiento de los modelos utilizando métricas apropiadas como la matriz de confusión, precisión, recall y F1-score.\n",
    "- **Validación Cruzada:** Utilizar la validación cruzada para evaluar la estabilidad y generalización del modelo en diferentes subconjuntos del conjunto de datos.\n",
    "\n",
    "## Siguientes Pasos en el Aprendizaje\n",
    "\n",
    "- **Técnicas Avanzadas:** Explorar técnicas más avanzadas de clasificación, como árboles de decisión, Random Forest y redes neuronales.\n",
    "- **Optimización de Hiperparámetros:** Aprender a utilizar técnicas como la búsqueda en cuadrícula (Grid Search) y la optimización bayesiana para encontrar los mejores hiperparámetros para los modelos.\n",
    "- **Trabajo con Datos Reales:** Aplicar estos modelos a conjuntos de datos más grandes y complejos para obtener una experiencia práctica más profunda.\n",
    "\n",
    "Esperamos que esta libreta te haya proporcionado una comprensión sólida de los algoritmos de clasificación básicos y cómo implementarlos utilizando scikit-learn. ¡Buena suerte en tu viaje de aprendizaje en ciencia de datos y aprendizaje automático!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
