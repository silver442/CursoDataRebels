{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae588ebe",
      "metadata": {},
      "source": [
        "# Acrividad Automatización de la recolección\n",
        "**Nombre:** Silvestre Hernandez Hernandez"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fddd7ab",
      "metadata": {},
      "source": [
        "### Extracción Automatizada de Datos de Archivos CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b56a549e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Nombre  Edad     Ciudad\n",
            "0    Ana    23     Madrid\n",
            "1   Luis    34  Barcelona\n",
            "2  Pedro    29   Valencia\n",
            "3  Marta    22    Sevilla\n",
            "4  Jorge    45     Bilbao\n",
            "5  Lucía    31   Zaragoza\n"
          ]
        }
      ],
      "source": [
        "# Extracción Automatizada de Datos de Archivos CSV\n",
        "\"\"\"\n",
        "Crea un script en Python que automatice la extracción de datos de varios archivos CSV almacenados en una carpeta, combinándolos \n",
        "en un único DataFrame de Pandas.\n",
        "Tips: Utiliza pandas para leer archivos CSV. Puedes usar os.listdir para listar los archivos en una carpeta.\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Definir la ruta de la carpeta que contiene los archivos CSV\n",
        "carpeta_csv = './csv_noCombinados'\n",
        "\n",
        "# Obtener una lista de todos los archivos en la carpeta\n",
        "archivos_csv = [archivo for archivo in os.listdir(carpeta_csv) if archivo.endswith('.csv')]\n",
        "\n",
        "# Creación de una lista para almacenar los DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Leer cada archivo CSV y agregar el DataFrame resultante a la lista\n",
        "for archivo in archivos_csv:\n",
        "    ruta_completa = os.path.join(carpeta_csv, archivo)\n",
        "    df = pd.read_csv(ruta_completa)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Combinar todos los DataFrames en uno solo\n",
        "df_combinado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Mostrar el DataFrame combinado\n",
        "print(df_combinado)\n",
        "\n",
        "# Opcional: Guardar el DataFrame combinado en un nuevo archivo CSV\n",
        "df_combinado.to_csv('./csv_conbinado/archivo_combinado.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89f0a26",
      "metadata": {},
      "source": [
        "### Automatización de la Limpieza de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b2032b7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valores faltantes por columna:\n",
            "id          0\n",
            "columna1    2\n",
            "columna2    3\n",
            "columna3    4\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Automatización de la Limpieza de Datos\n",
        "\"\"\"\n",
        "Diseña un programa en Python que identifique y corrija valores atípicos y faltantes en un conjunto de datos proporcionado, utilizando Pandas.\n",
        "Tips: Explora métodos como fillna() y dropna() para manejar valores faltantes y considera técnicas estadísticas para detectar valores atípicos.\n",
        "\"\"\"\n",
        "# Espacio para la solución del usuario:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Paso 2: Cargar el conjunto de datos que esta en una carpeta csv en un archivo llamado 'EjemploCsv.csv'\n",
        "df = pd.read_csv('./csv/EjemploCsv.csv')\n",
        "\n",
        "# Paso 3: Identificar y manejar valores faltantes\n",
        "# Contar los valores faltantes por columna\n",
        "print(\"Valores faltantes por columna:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# imputar los valores faltantes en columnas numéricas con la media\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Opcionalmente se podría eliminar filas con valores faltantes\n",
        "# df.dropna(inplace=True)\n",
        "\n",
        "# Paso 4: Detectar y corregir valores atípicos\n",
        "# usar el método del rango intercuartílico (IQR) para identificar outliers en una columna 'columna1'\n",
        "Q1 = df['columna1'].quantile(0.25)\n",
        "Q3 = df['columna1'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "filtro_outliers = (df['columna1'] < Q1 - 1.5 * IQR) | (df['columna1'] > Q3 + 1.5 * IQR)\n",
        "\n",
        "# Opciones para manejar outliers, reemplazando con NaN o con el valor medio\n",
        "df.loc[filtro_outliers, 'columna1'] = np.nan\n",
        "# df['columna1'].fillna(df['columna1'].mean(), inplace=True)\n",
        "\n",
        "# Guardar el DataFrame limpio en un nuevo archivo CSV\n",
        "df.to_csv('./csv/datos_limpio.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557d4885",
      "metadata": {},
      "source": [
        "### Integración de Datos de Diversas Fuentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e99be83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id nombre  edad     ciudad  salario\n",
            "0   1   Juan    28     Madrid   3000.0\n",
            "1   2    Ana    24  Barcelona   3200.0\n",
            "2   3   Luis    35   Valencia   2800.0\n",
            "3   4  Maria    30    Sevilla   3100.0\n",
            "4   5  Pedro    22     Bilbao   3300.0\n"
          ]
        }
      ],
      "source": [
        "# Integración de Datos de Diversas Fuentes\n",
        "\"\"\"\n",
        "Utiliza Python para extraer datos de al menos dos fuentes diferentes (por ejemplo, un archivo CSV y una base de datos SQL), y combínalos \n",
        "en un único DataFrame.\n",
        "Tips: Para la extracción de bases de datos SQL, utiliza SQLAlchemy o pandas read_sql. Usa merge() o concat() para combinar DataFrames.\n",
        "\"\"\"\n",
        "# Espacio para la solución del usuario:\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Leer el archivo CSV\n",
        "df_csv = pd.read_csv('./csv/datos.csv')\n",
        "\n",
        "# Conectar a la base de datos SQL y leer datos\n",
        "engine = create_engine('sqlite:///Base_datos/basededatos.db')\n",
        "df_sql = pd.read_sql('SELECT * FROM nombretabla', engine)\n",
        "\n",
        "# Combinar los DataFrames utilizando la columna 'id'\n",
        "df_combinado = pd.merge(df_csv, df_sql, on='id')\n",
        "\n",
        "# Mostrar el DataFrame combinado\n",
        "print(df_combinado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e110b5",
      "metadata": {},
      "source": [
        "### Automatización de Extracción de Datos de API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2c30d761",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id              name   username                      email  \\\n",
            "0   1     Leanne Graham       Bret          Sincere@april.biz   \n",
            "1   2      Ervin Howell  Antonette          Shanna@melissa.tv   \n",
            "2   3  Clementine Bauch   Samantha         Nathan@yesenia.net   \n",
            "3   4  Patricia Lebsack   Karianne  Julianne.OConner@kory.org   \n",
            "4   5  Chelsey Dietrich     Kamren   Lucio_Hettinger@annie.ca   \n",
            "\n",
            "                                             address                  phone  \\\n",
            "0  {'street': 'Kulas Light', 'suite': 'Apt. 556',...  1-770-736-8031 x56442   \n",
            "1  {'street': 'Victor Plains', 'suite': 'Suite 87...    010-692-6593 x09125   \n",
            "2  {'street': 'Douglas Extension', 'suite': 'Suit...         1-463-123-4447   \n",
            "3  {'street': 'Hoeger Mall', 'suite': 'Apt. 692',...      493-170-9623 x156   \n",
            "4  {'street': 'Skiles Walks', 'suite': 'Suite 351...          (254)954-1289   \n",
            "\n",
            "         website                                            company  \n",
            "0  hildegard.org  {'name': 'Romaguera-Crona', 'catchPhrase': 'Mu...  \n",
            "1  anastasia.net  {'name': 'Deckow-Crist', 'catchPhrase': 'Proac...  \n",
            "2    ramiro.info  {'name': 'Romaguera-Jacobson', 'catchPhrase': ...  \n",
            "3       kale.biz  {'name': 'Robel-Corkery', 'catchPhrase': 'Mult...  \n",
            "4   demarco.info  {'name': 'Keebler LLC', 'catchPhrase': 'User-c...  \n"
          ]
        }
      ],
      "source": [
        "# Automatización de Extracción de Datos de API\n",
        "\"\"\"\n",
        "Escribe un script en Python que extraiga datos de una API pública y los transforme en un formato utilizable para el análisis.\n",
        "Tips: Utiliza la librería requests para hacer solicitudes a la API y json para procesar la respuesta.\n",
        "\"\"\"\n",
        "# Espacio para la solución del usuario:\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Paso 1: Hacer una solicitud a la API\n",
        "response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
        "\n",
        "# Paso 2: Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Paso 3: Procesar la respuesta en formato JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Paso 4: Transformar los datos en un DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Paso 5: Mostrar el DataFrame\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f'Error: {response.status_code}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9d54ca",
      "metadata": {},
      "source": [
        "### Simulación de Automatización de Preparación de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a9e5f3b8",
      "metadata": {
        "id": "a9e5f3b8"
      },
      "outputs": [],
      "source": [
        "# Simulación de Automatización de Preparación de Datos\n",
        "\"\"\"\n",
        "Crea un flujo de trabajo automatizado que simule la recepción de nuevos datos diariamente, limpie y transforme estos datos, y \n",
        "los almacene para análisis futuros.\n",
        "Tips: Implementa un script que simule la adquisición de nuevos datos y usa pandas para las operaciones de limpieza y transformación.\n",
        "\"\"\"\n",
        "# Espacio para la solución del usuario:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Función para simular la adquisición de nuevos datos\n",
        "def adquirir_datos():\n",
        "    # Simular datos con valores aleatorios\n",
        "    np.random.seed(0)\n",
        "    data = {\n",
        "        'fecha': [datetime.now().strftime('%Y-%m-%d')] * 10,\n",
        "        'valor1': np.random.randint(0, 100, 10),\n",
        "        'valor2': np.random.randn(10) * 100,\n",
        "        'categoria': np.random.choice(['A', 'B', 'C'], 10)\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Función para limpiar los datos\n",
        "def limpiar_datos(df):\n",
        "    # Eliminar filas con valores nulos\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Convertir la columna 'fecha' a tipo datetime\n",
        "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para transformar los datos\n",
        "def transformar_datos(df):\n",
        "    # Crear una nueva columna basada en una condición\n",
        "    df['valor1_categoria'] = df['valor1'] * df['categoria'].map({'A': 1, 'B': 2, 'C': 3})\n",
        "\n",
        "    # Agregar columnas adicionales si es necesario\n",
        "    df['valor2_abs'] = df['valor2'].abs()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para almacenar los datos\n",
        "def almacenar_datos(df, nombre_archivo='csv/datos_limpios.csv'):\n",
        "    # Guardar el DataFrame en un archivo CSV\n",
        "    df.to_csv(nombre_archivo, index=False)\n",
        "\n",
        "# Flujo de trabajo automatizado\n",
        "def flujo_trabajo():\n",
        "    # Simular la adquisición de nuevos datos diariamente\n",
        "    nuevos_datos = adquirir_datos()\n",
        "\n",
        "    # Limpiar los datos\n",
        "    datos_limpios = limpiar_datos(nuevos_datos)\n",
        "\n",
        "    # Transformar los datos\n",
        "    datos_transformados = transformar_datos(datos_limpios)\n",
        "\n",
        "    # Almacenar los datos para análisis futuros\n",
        "    almacenar_datos(datos_transformados)\n",
        "\n",
        "# Ejecutar el flujo de trabajo\n",
        "if __name__ == \"__main__\":\n",
        "    flujo_trabajo()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
