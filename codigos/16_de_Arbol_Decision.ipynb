{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03051609",
      "metadata": {
        "id": "03051609"
      },
      "source": [
        "# 1. Introducción a los Árboles de Decisión\n",
        "\n",
        "Los árboles de decisión son una técnica de aprendizaje supervisado que se utiliza tanto en problemas de clasificación como de regresión. Son parte de una clase de algoritmos llamados modelos de caja blanca, lo que significa que son fáciles de entender e interpretar ya que reflejan claramente el proceso de toma de decisiones.\n",
        "\n",
        "La idea central detrás de un árbol de decisión es dividir los datos en múltiples conjuntos basados en diferentes condiciones hasta alcanzar un conjunto que tenga la mayor pureza posible, es decir, que contenga la mayor cantidad posible de elementos de una misma clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d38ad38",
      "metadata": {
        "id": "2d38ad38"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "# Cargar el conjunto de datos Iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Crear y entrenar el modelo de árbol de decisión\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Visualizar el árbol de decisión\n",
        "plt.figure(figsize=(15,10))\n",
        "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.title(\"Árbol de Decisión - Ejemplo con el Conjunto de Datos Iris\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048d06bb",
      "metadata": {
        "id": "048d06bb"
      },
      "source": [
        "# 2. Componentes de un Árbol de Decisión\n",
        "\n",
        "Un árbol de decisión se compone de los siguientes elementos:\n",
        "\n",
        "## 2.1 Nodos de decisión\n",
        "Estos son los nodos que contienen una pregunta o una condición que divide los datos. A partir de cada nodo de decisión, se extienden dos o más ramas, cada una representando posibles respuestas a la pregunta o condiciones.\n",
        "\n",
        "## 2.2 Ramas\n",
        "Las ramas representan el resultado de una prueba en un nodo de decisión. En otras palabras, son las \"respuestas\" a la pregunta o condición en el nodo de decisión. Cada rama conecta dos nodos: un nodo de decisión y otro nodo de decisión o un nodo de hoja.\n",
        "\n",
        "## 2.3 Nodos de hoja\n",
        "Los nodos de hoja, también conocidos como nodos terminales, son los nodos donde termina el camino y no hay más preguntas. Representan la decisión final o el resultado de la clasificación o regresión.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed376fad",
      "metadata": {
        "id": "ed376fad"
      },
      "source": [
        "## 2.4 Ejemplo\n",
        "\n",
        "Veamos un ejemplo sencillo. Imaginemos que tenemos un árbol de decisión con la siguiente pregunta en el nodo de decisión inicial: \"¿Está lloviendo?\" Esta es una condición que divide nuestros datos en dos: los días en que está lloviendo y los días en que no lo está.\n",
        "\n",
        "Las ramas que se extienden desde este nodo representan las posibles respuestas: una rama para \"Sí\" y otra para \"No\".\n",
        "\n",
        "Si seguimos la rama \"Sí\", podríamos llegar a un nodo de hoja que dice \"Llevar paraguas\". No hay más preguntas después de esto, así que este es nuestro resultado final. Del mismo modo, si seguimos la rama \"No\", podríamos llegar a un nodo de hoja que dice \"No llevar paraguas\".\n",
        "\n",
        "Por supuesto, en la práctica, los árboles de decisión pueden ser mucho más complicados que esto, con muchos más nodos de decisión y nodos de hoja, dependiendo de cuántas características estés considerando y cuán detalladas sean tus decisiones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987deee8",
      "metadata": {
        "id": "987deee8"
      },
      "source": [
        "# 3. Cómo se crea un árbol de decisión: Algoritmo ID3\n",
        "\n",
        "Para construir un árbol de decisión, necesitamos un proceso sistemático para determinar qué preguntas hacer y en qué orden. Uno de los algoritmos más conocidos para hacer esto es el algoritmo ID3.\n",
        "\n",
        "El algoritmo ID3, que significa Iterative Dichotomiser 3, fue desarrollado por Ross Quinlan. Se utiliza para generar un árbol de decisión a partir de un conjunto de datos. El algoritmo ID3 sigue un enfoque de tipo 'codicioso' en el sentido de que hace la elección óptima local en cada nodo con la esperanza de que estas elecciones locales lleven a una solución global óptima. El algoritmo hace esto de la siguiente manera:\n",
        "\n",
        "1. Comienza con el conjunto de datos original como la raíz del árbol.\n",
        "2. Si todas las instancias en el conjunto de datos son de la misma clase, entonces este nodo se convierte en un nodo hoja y se etiqueta con esa clase.\n",
        "3. Si no, calcula la entropía del conjunto de datos.\n",
        "4. Para cada característica en el conjunto de datos, calcula la ganancia de información de dividir el conjunto de datos en subconjuntos según esa característica.\n",
        "5. Elige la característica que tiene la mayor ganancia de información.\n",
        "6. Divide el conjunto de datos en subconjuntos según la característica seleccionada.\n",
        "7. Repite el proceso para cada subconjunto, creando un nuevo nodo de decisión para cada uno.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa48ae0c",
      "metadata": {
        "id": "fa48ae0c"
      },
      "source": [
        "## 3.1 Cálculo de la Entropía y la Ganancia de Información\n",
        "\n",
        "### Entropía\n",
        "\n",
        "La entropía es una medida de la impureza o el desorden. En el contexto de los árboles de decisión, la entropía se utiliza para medir la impureza de una entrada de datos.\n",
        "\n",
        "Para calcular la entropía de un conjunto de datos, usamos la fórmula:\n",
        "\n",
        "$Entropía(S) = - \\sum_{i} p_i \\cdot log_2(p_i)$\n",
        "\n",
        "Donde:\n",
        "- $S$ es el conjunto total de ejemplos\n",
        "- $p_i$ es la proporción de ejemplos que pertenecen a la etiqueta $i$ en el conjunto $S$.\n",
        "\n",
        "La entropía será 0 cuando todos los ejemplos en $S$ son de la misma clase, y será 1 cuando los ejemplos están igualmente divididos entre todas las clases.\n",
        "\n",
        "### Ganancia de Información\n",
        "\n",
        "La ganancia de información es una métrica que nos ayuda a decidir cuál característica es la mejor para dividir un nodo en un árbol de decisión.\n",
        "\n",
        "Para calcular la ganancia de información de una característica, usamos la fórmula:\n",
        "\n",
        "$Ganancia(S, A) = Entropía(S) - \\sum_{v \\in Valores(A)} \\left( \\frac{|S_v|}{|S|} \\cdot Entropía(S_v) \\right)$\n",
        "\n",
        "Donde:\n",
        "- $S$ es el conjunto total de ejemplos\n",
        "- $A$ es la característica que estamos considerando para dividir\n",
        "- $Valores(A)$ son todos los posibles valores de la característica $A$\n",
        "- $S_v$ es el subconjunto de ejemplos en $S$ donde la característica $A$ tiene el valor $v$\n",
        "- $|S_v|$ y $|S|$ son las cantidades de ejemplos en los conjuntos $S_v$ y $S$ respectivamente.\n",
        "\n",
        "La idea es que queremos dividir por la característica que más reduzca la impureza (según la entropía) de nuestros conjuntos resultantes. Por lo tanto, buscamos la característica que nos dé la mayor ganancia de información.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed12dbcd",
      "metadata": {
        "id": "ed12dbcd"
      },
      "source": [
        "## 3.2 Ejemplo manual de cómo se divide una variable en ramas usando entropía y ganancia de información\n",
        "\n",
        "Consideremos un conjunto de datos muy sencillo sobre jugar al tenis, que incluye características como el pronóstico del clima (soleado, nublado, lluvioso), el viento (débil, fuerte) y si jugamos al tenis o no en esas condiciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653f4037",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:14:55.798084Z",
          "start_time": "2023-06-09T20:14:55.763820Z"
        },
        "id": "653f4037"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Crear el conjunto de datos\n",
        "data = pd.DataFrame({\n",
        "    'Clima': ['Soleado', 'Nublado', 'Lluvioso', 'Soleado', 'Nublado', 'Soleado', 'Lluvioso', 'Soleado'],\n",
        "    'Viento': ['Fuerte', 'Débil', 'Fuerte', 'Fuerte', 'Débil', 'Débil', 'Fuerte', 'Débil'],\n",
        "    'Jugar': ['No', 'Si', 'No', 'Si', 'Si', 'No', 'No', 'Si']\n",
        "})\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc1359f",
      "metadata": {
        "id": "9cc1359f"
      },
      "source": [
        "Después, calcularemos la entropía del conjunto de datos completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfbaa58e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:25:10.348621Z",
          "start_time": "2023-06-09T20:25:10.305624Z"
        },
        "id": "bfbaa58e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Calcular la entropía del conjunto de datos completo\n",
        "total = len(data)\n",
        "jugar_counts = data['Jugar'].value_counts()\n",
        "jugar_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02fe35c1",
      "metadata": {
        "id": "02fe35c1"
      },
      "outputs": [],
      "source": [
        "total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66bd049b",
      "metadata": {
        "id": "66bd049b"
      },
      "outputs": [],
      "source": [
        "entropia_total = -sum([(count/total)*math.log2(count/total) for count in jugar_counts])\n",
        "print(\"Entropía total:\", entropia_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d10332c",
      "metadata": {
        "id": "7d10332c"
      },
      "source": [
        "A continuación, vamos a calcular la ganancia de información para las variables `Clima` y `Viento`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df22cdef",
      "metadata": {
        "id": "df22cdef"
      },
      "outputs": [],
      "source": [
        "var_values = data['Clima'].unique()\n",
        "var_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbe98d7",
      "metadata": {
        "id": "4bbe98d7"
      },
      "outputs": [],
      "source": [
        "data[data['Clima'] == 'Soleado']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585e1446",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:25:14.614522Z",
          "start_time": "2023-06-09T20:25:14.592113Z"
        },
        "id": "585e1446"
      },
      "outputs": [],
      "source": [
        "# Calcular la ganancia de información para las variables\n",
        "variables = ['Clima', 'Viento']\n",
        "\n",
        "for var in variables:\n",
        "    var_values = data[var].unique()\n",
        "    entropias_var = []\n",
        "\n",
        "    for value in var_values:\n",
        "        subdata = data[data[var] == value]\n",
        "        subdata_total = len(subdata)\n",
        "        jugar_counts = subdata['Jugar'].value_counts()\n",
        "        entropia_subdata = -sum([ (count/subdata_total)*math.log2(count/subdata_total) for count in jugar_counts ])\n",
        "        entropias_var.append((subdata_total/total)*entropia_subdata)\n",
        "\n",
        "    ganancia_var = entropia_total - sum(entropias_var)\n",
        "    print(f\"Ganancia de información para '{var}':\", ganancia_var)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e6f7055",
      "metadata": {
        "id": "5e6f7055"
      },
      "source": [
        "Después de calcular la ganancia de información para las variables `Clima` y `Viento`, podemos ver que la variable `Clima` tiene la mayor ganancia de información. Por lo tanto, `Clima` es la variable que deberíamos seleccionar para dividir nuestro conjunto de datos en este punto.\n",
        "\n",
        "Esto significa que el primer nodo en nuestro árbol de decisión debería ser una división basada en la variable `Clima`. Los siguientes nodos del árbol se crean siguiendo el mismo proceso para los subconjuntos de datos resultantes.\n",
        "\n",
        "Es importante recordar que este es un proceso iterativo. Después de dividir por `Clima`, tendríamos que calcular de nuevo la ganancia de información para todas las variables en cada uno de los subconjuntos de datos y seleccionar la variable con mayor ganancia de información para la siguiente división. Este proceso se repite hasta que todos los elementos dentro de un subconjunto pertenezcan a la misma clase, o hasta que no nos queden más variables por las cuales dividir."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c33a06",
      "metadata": {
        "id": "65c33a06"
      },
      "source": [
        "# 4. Implementación de un árbol de decisión simple en Python\n",
        "\n",
        "Vamos a implementar un árbol de decisión utilizando la biblioteca `sklearn` y el conjunto de datos `Iris`.\n",
        "\n",
        "Primero, importamos las librerías necesarias y cargamos el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d308bfd0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:32:31.391767Z",
          "start_time": "2023-06-09T20:32:28.343786Z"
        },
        "id": "d308bfd0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargamos el conjunto de datos Iris\n",
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac78c728",
      "metadata": {
        "id": "ac78c728"
      },
      "outputs": [],
      "source": [
        "iris.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3748149",
      "metadata": {
        "id": "e3748149"
      },
      "outputs": [],
      "source": [
        "iris.target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ea3c42",
      "metadata": {
        "id": "04ea3c42"
      },
      "outputs": [],
      "source": [
        "iris.feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbcac485",
      "metadata": {
        "id": "bbcac485"
      },
      "source": [
        "A continuación, dividimos los datos en un conjunto de entrenamiento y otro de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f1862d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:32:44.447079Z",
          "start_time": "2023-06-09T20:32:44.427048Z"
        },
        "id": "07f1862d"
      },
      "outputs": [],
      "source": [
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
        "                                                    iris.target,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10bcd77",
      "metadata": {
        "id": "f10bcd77"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b9ed4f",
      "metadata": {
        "id": "b8b9ed4f"
      },
      "source": [
        "Luego, entrenamos un árbol de decisión con el conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6334038",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:32:57.047016Z",
          "start_time": "2023-06-09T20:32:57.025017Z"
        },
        "id": "e6334038"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el árbol de decisión\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f10e23",
      "metadata": {
        "id": "a1f10e23"
      },
      "source": [
        "Después, podemos visualizar el árbol de decisión que hemos entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514ad262",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:33:13.841256Z",
          "start_time": "2023-06-09T20:33:13.192043Z"
        },
        "id": "514ad262"
      },
      "outputs": [],
      "source": [
        "# Visualizamos el árbol de decisión\n",
        "plt.figure(figsize=(15,10))\n",
        "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.show()\n",
        "# sepal_length=2.4, sepal_width=3.5, petal_length=3.4, petal_width=2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1407c382",
      "metadata": {
        "id": "1407c382"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "clf.predict(np.array([[2.4,3.5,3.4,2.0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c826dac",
      "metadata": {
        "id": "3c826dac"
      },
      "outputs": [],
      "source": [
        "iris.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbb9805",
      "metadata": {
        "id": "9dbb9805"
      },
      "source": [
        "Por último, podemos usar nuestro árbol de decisión para hacer predicciones en el conjunto de prueba y evaluar su rendimiento.\n",
        "\n",
        "# 5. Métricas de evaluación de rendimiento\n",
        "\n",
        "## Matriz de Confusión: Verdaderos Positivos, Verdaderos Negativos, Falsos Positivos y Falsos Negativos\n",
        "\n",
        "Para comprender completamente las métricas de precisión, recall y accuracy, es útil primero entender la matriz de confusión. Una matriz de confusión es una tabla que se utiliza para describir el rendimiento de un modelo de clasificación en un conjunto de datos para los cuales los valores verdaderos son conocidos. Se llama así porque hace muy fácil ver si el sistema está confundiendo dos clases.\n",
        "\n",
        "En esta matriz:\n",
        "\n",
        "- **Verdaderos Positivos (TP):** Son aquellos casos en los que el modelo predijo 'positivo' y la clase verdadera también es 'positiva'.\n",
        "\n",
        "- **Verdaderos Negativos (TN):** Son aquellos casos en los que el modelo predijo 'negativo' y la clase verdadera también es 'negativa'.\n",
        "\n",
        "- **Falsos Positivos (FP):** Son aquellos casos en los que el modelo predijo 'positivo' pero la clase verdadera es 'negativa'. También se conocen como \"Errores de Tipo I\".\n",
        "\n",
        "- **Falsos Negativos (FN):** Son aquellos casos en los que el modelo predijo 'negativo' pero la clase verdadera es 'positiva'. También se conocen como \"Errores de Tipo II\".\n",
        "\n",
        "Estos cuatro números forman la matriz de confusión y son la base para varias métricas importantes para evaluar el rendimiento de los modelos de clasificación.\n",
        "\n",
        "\n",
        "### Precisión (Precision)\n",
        "\n",
        "La precisión se calcula como el número de Verdaderos Positivos (TP) dividido por la suma de Verdaderos Positivos (TP) y Falsos Positivos (FP). Es una indicación de cuántas de las clasificaciones positivas del modelo son realmente correctas.\n",
        "\n",
        "$Precisión = \\frac{TP}{TP+FP}$\n",
        "\n",
        "### Recall (Sensibilidad)\n",
        "\n",
        "El recall, también conocido como sensibilidad, se calcula como el número de Verdaderos Positivos (TP) dividido por la suma de Verdaderos Positivos (TP) y Falsos Negativos (FN). Es una indicación de cuántos de los positivos reales el modelo es capaz de identificar.\n",
        "\n",
        "$Recall = \\frac{TP}{TP+FN}$\n",
        "\n",
        "### F1-Score\n",
        "\n",
        "El F1-Score es la media armónica de la precisión y el recall, y proporciona una medida única que equilibra ambas métricas. Un F1-Score perfecto sería 1, lo que indica precisión y recall perfectos, y el peor sería 0.\n",
        "\n",
        "$F1-Score = 2 \\cdot \\frac{Precisión \\cdot Recall}{Precisión + Recall}$\n",
        "\n",
        "### Support\n",
        "\n",
        "El support es simplemente el número de ocurrencias de la clase verdadera en el conjunto de datos. En el caso de un conjunto de datos equilibrado, este número debería ser el mismo para cada clase.\n",
        "\n",
        "### Accuracy (Exactitud)\n",
        "\n",
        "La accuracy es quizás la métrica más intuitiva. Se calcula como el número de predicciones correctas (Verdaderos Positivos y Verdaderos Negativos) dividido por el número total de observaciones. En otras palabras, es la proporción de predicciones correctas que hizo nuestro modelo.\n",
        "\n",
        "$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$\n",
        "\n",
        "Es importante señalar que aunque la accuracy puede ser una medida útil en muchos casos, puede ser engañosa si las clases están muy desequilibradas. Por ejemplo, si el 95% de tus datos son de la clase A y sólo el 5% son de la clase B, un modelo que siempre predice la clase A tendrá una accuracy del 95%, pero no será un buen modelo ya que es incapaz de identificar correctamente los ejemplos de la clase B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0892d3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-09T20:33:41.564770Z",
          "start_time": "2023-06-09T20:33:41.547770Z"
        },
        "id": "0b0892d3"
      },
      "outputs": [],
      "source": [
        "# Hacemos predicciones en el conjunto de prueba\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluamos el rendimiento del árbol\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36997c5",
      "metadata": {
        "id": "f36997c5"
      },
      "source": [
        "# 6. Ejemplo de Árbol de Decisión con otro Dataset\n",
        "\n",
        "En esta sección, realizaremos un análisis utilizando el conjunto de datos Titanic. Este conjunto de datos contiene información sobre los pasajeros del Titanic, como si sobrevivieron o no, la clase en la que viajaban, su sexo, edad, entre otros.\n",
        "\n",
        "## 6.1 Descripción del Dataset Titanic\n",
        "\n",
        "El conjunto de datos Titanic contiene las siguientes variables:\n",
        "\n",
        "- `survived`: Indica si el pasajero sobrevivió (1) o no (0).\n",
        "- `pclass`: Clase del pasajero (1 = Primera, 2 = Segunda, 3 = Tercera).\n",
        "- `sex`: Sexo del pasajero (male o female).\n",
        "- `age`: Edad del pasajero.\n",
        "- `sibsp`: Número de hermanos/esposos a bordo.\n",
        "- `parch`: Número de padres/hijos a bordo.\n",
        "- `fare`: Tarifa pagada por el pasajero.\n",
        "- `embarked`: Puerto de embarque (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
        "- `class`: Clase del pasajero (First, Second, Third), derivada de `pclass`.\n",
        "- `who`: Descripción de la persona (man, woman, child).\n",
        "- `adult_male`: Indica si el pasajero es un hombre adulto.\n",
        "- `deck`: Cubierta en la que se encontraba el pasajero.\n",
        "- `embark_town`: Nombre del puerto de embarque.\n",
        "- `alive`: Indica si el pasajero sobrevivió (yes) o no (no), derivada de `survived`.\n",
        "- `alone`: Indica si el pasajero viajaba solo.\n",
        "\n",
        "## 6.2 Manejo de Valores Nulos\n",
        "\n",
        "Para manejar los valores nulos en el dataset Titanic, realizamos las siguientes acciones:\n",
        "\n",
        "- Rellenamos los valores nulos en la columna `age` con la media de la columna.\n",
        "- Rellenamos los valores nulos en las columnas `embarked` y `embark_town` con la moda de cada columna.\n",
        "- Añadimos la categoría 'Unknown' a la columna `deck` y rellenamos sus valores nulos con esta nueva categoría.\n",
        "\n",
        "## 6.3 Aplicación de One-Hot Encoding\n",
        "\n",
        "One-Hot Encoding es una técnica utilizada para convertir variables categóricas en una forma que pueda ser proporcionada a algoritmos de aprendizaje automático. La mayoría de los algoritmos no pueden manejar variables categóricas directamente, ya que se basan en operaciones matemáticas que no pueden ser realizadas en datos categóricos. One-Hot Encoding permite representar estas variables de una manera que puede ser interpretada por los algoritmos.\n",
        "\n",
        "### ¿Cómo funciona One-Hot Encoding?\n",
        "\n",
        "Supongamos que tenemos una variable categórica `embarked` con tres categorías: `C`, `Q` y `S`. One-Hot Encoding convertirá esta variable en tres columnas binarias (0 o 1), una para cada categoría:\n",
        "\n",
        "| embarked_C | embarked_Q | embarked_S |\n",
        "|------------|------------|------------|\n",
        "|      1     |      0     |      0     |\n",
        "|      0     |      1     |      0     |\n",
        "|      0     |      0     |      1     |\n",
        "\n",
        "Esto asegura que el algoritmo no asuma una relación ordinal entre las categorías.\n",
        "\n",
        "A continuación, aplicamos One-Hot Encoding a todas las variables categóricas relevantes del conjunto de datos Titanic, excluyendo las derivadas repetidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4957eb5e",
      "metadata": {
        "id": "4957eb5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cargar el conjunto de datos Titanic\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Seleccionar características y etiqueta\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alone']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Manejar valores nulos\n",
        "# Rellenar valores nulos en 'age' con la media\n",
        "X['age'].fillna(X['age'].mean(), inplace=True)\n",
        "\n",
        "# Rellenar valores nulos en 'embarked' y 'embark_town' con la moda\n",
        "X['embarked'].fillna(X['embarked'].mode()[0], inplace=True)\n",
        "X['embark_town'].fillna(X['embark_town'].mode()[0], inplace=True)\n",
        "\n",
        "# Añadir la categoría 'Unknown' a la columna 'deck'\n",
        "X['deck'] = X['deck'].cat.add_categories(['Unknown'])\n",
        "# Rellenar valores nulos en 'deck' con la categoría 'Unknown'\n",
        "X['deck'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Aplicar One-Hot Encoding a las variables categóricas\n",
        "X_encoded = pd.get_dummies(X, columns=['pclass', 'sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alone'],\n",
        "                           drop_first=True)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame transformado\n",
        "X_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a75d126",
      "metadata": {
        "id": "2a75d126"
      },
      "outputs": [],
      "source": [
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30294f0f",
      "metadata": {
        "id": "30294f0f"
      },
      "source": [
        "## 6.4 Parámetros Importantes de un Árbol de Decisión\n",
        "\n",
        "Al construir un modelo de árbol de decisión, hay varios parámetros importantes que pueden influir en el rendimiento del modelo. Aquí se describen algunos de los más relevantes:\n",
        "\n",
        "- **criterion**: La función que se utiliza para medir la calidad de una división. Las opciones son \"gini\" para el índice de Gini y \"entropy\" para la ganancia de información.\n",
        "- **splitter**: La estrategia utilizada para elegir la división en cada nodo. Las opciones son \"best\" para elegir la mejor división y \"random\" para elegir la mejor división aleatoria.\n",
        "- **max_depth**: La profundidad máxima del árbol. Limitar la profundidad del árbol puede ayudar a prevenir el sobreajuste (overfitting).\n",
        "- **min_samples_split**: El número mínimo de muestras necesarias para dividir un nodo.\n",
        "- **min_samples_leaf**: El número mínimo de muestras que debe tener un nodo hoja.\n",
        "- **max_features**: El número máximo de características que se consideran para dividir un nodo.\n",
        "\n",
        "A continuación, ajustamos el modelo utilizando el parámetro `max_depth` para controlar el tamaño del árbol.\n",
        "\n",
        "## 6.5 Creación y Visualización del Modelo con Parámetros Ajustados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417d220b",
      "metadata": {
        "id": "417d220b"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear y entrenar el modelo de árbol de decisión con varios parámetros ajustados\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='entropy',        # Usar la ganancia de información\n",
        "    splitter='best',            # Elegir la mejor división\n",
        "    max_depth=4,                # Profundidad máxima del árbol\n",
        "    min_samples_split=10,       # Número mínimo de muestras para dividir un nodo\n",
        "    min_samples_leaf=5,         # Número mínimo de muestras que debe tener un nodo hoja\n",
        "    max_features=None           # Considerar todas las características para dividir un nodo\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualizar el árbol de decisión\n",
        "plt.figure(figsize=(15,10))\n",
        "plot_tree(clf, filled=True, feature_names=X_encoded.columns, class_names=['Not Survived', 'Survived'])\n",
        "plt.title(\"Árbol de Decisión - Ejemplo con el Conjunto de Datos Titanic\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de311ff",
      "metadata": {
        "id": "5de311ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Mostrar los resultados de la evaluación\n",
        "print(f\"Exactitud: {accuracy}\")\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(conf_matrix)\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d2b5be",
      "metadata": {
        "id": "33d2b5be"
      },
      "source": [
        "## 7. Conclusión\n",
        "\n",
        "En esta libreta, hemos explorado los árboles de decisión como una técnica de aprendizaje supervisado, tanto para problemas de clasificación como de regresión. Comenzamos con una introducción teórica a los árboles de decisión, sus componentes y su funcionamiento básico. Posteriormente, aplicamos este conocimiento en un ejemplo práctico utilizando el conjunto de datos Titanic.\n",
        "\n",
        "Realizamos los siguientes pasos:\n",
        "\n",
        "1. **Carga y Preparación de Datos**: Seleccionamos las características relevantes, manejamos valores nulos y aplicamos One-Hot Encoding a las variables categóricas para que puedan ser utilizadas por el modelo.\n",
        "2. **Creación y Entrenamiento del Modelo**: Utilizamos `DecisionTreeClassifier` de `scikit-learn` para construir un modelo de árbol de decisión, ajustando varios parámetros importantes para mejorar su rendimiento.\n",
        "3. **Visualización y Evaluación del Modelo**: Visualizamos el árbol de decisión resultante y evaluamos su rendimiento utilizando métricas como la exactitud, la matriz de confusión y el reporte de clasificación.\n",
        "\n",
        "Al ajustar los parámetros del árbol de decisión, logramos controlar su complejidad y mejorar su capacidad para generalizar a nuevos datos. Este proceso demostró cómo los árboles de decisión pueden ser una herramienta poderosa y fácilmente interpretable para resolver problemas de clasificación.\n",
        "\n",
        "En resumen, los árboles de decisión son modelos flexibles y efectivos que, con el ajuste adecuado de sus parámetros, pueden proporcionar resultados precisos y comprensibles en una variedad de aplicaciones. Esperamos que esta libreta haya proporcionado una comprensión clara de cómo funcionan los árboles de decisión y cómo pueden aplicarse en análisis de datos reales."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}